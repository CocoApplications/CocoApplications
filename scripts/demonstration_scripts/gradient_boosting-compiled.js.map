{"version":3,"sources":["gradient_boosting.js"],"names":[],"mappings":"AAAA;;;;;;;;IAEM,qB;;;AAEF,mCAAY,CAAZ,EAAe,CAAf,EAAkB,SAAlB,EAA6B;AAAA;;AACzB,YAAI,aAAa,CAAb,IAAkB,EAAE,MAAF,GAAW,CAAjC,EAAoC;AAChC,iBAAK,OAAL,GAAe,IAAf;AACA,iBAAK,eAAL,GAAuB,MAAM,YAAN,CAAmB,CAAnB,CAAvB;AACH,SAHD,MAGO;AACH,iBAAK,OAAL,GAAe,KAAf;AACA,gBAAI,kBAAkB,sBAAsB,qBAAtB,CAA4C,CAA5C,EAA+C,CAA/C,CAAtB;AACA,iBAAK,aAAL,GAAqB,gBAAgB,CAAhB,CAArB;AACA,iBAAK,WAAL,GAAmB,gBAAgB,CAAhB,CAAnB;AACA,gBAAI,SAAS,EAAb;AACA,gBAAI,SAAS,EAAb;AACA,gBAAI,UAAU,EAAd;AACA,gBAAI,UAAU,EAAd;;AAEA,iBAAK,IAAI,WAAW,CAApB,EAAuB,WAAW,EAAE,MAApC,EAA4C,UAA5C,EAAwD;AACpD,oBAAI,UAAU,EAAE,QAAF,CAAd;AACA,oBAAI,UAAU,EAAE,QAAF,CAAd;AACA,oBAAI,KAAK,eAAL,CAAqB,OAArB,KAAiC,CAArC,EAAwC;AACpC,4BAAQ,IAAR,CAAa,OAAb;AACA,4BAAQ,IAAR,CAAa,OAAb;AACH,iBAHD,MAGO;AACH,2BAAO,IAAP,CAAY,OAAZ;AACA,2BAAO,IAAP,CAAY,OAAZ;AACH;AACJ;AACD,iBAAK,UAAL,GAAkB,IAAI,qBAAJ,CAA0B,MAA1B,EAAkC,MAAlC,EAA0C,YAAY,CAAtD,CAAlB;AACA,iBAAK,WAAL,GAAmB,IAAI,qBAAJ,CAA0B,OAA1B,EAAmC,OAAnC,EAA4C,YAAY,CAAxD,CAAnB;AACH;AACJ;;;;oDAE2B,C,EAAG,a,EAAe,Q,EAAS;AACnD,gBAAG,KAAK,OAAR,EAAgB;AACZ,qBAAK,eAAL,GAAuB,MAAM,WAAN,CAAkB,aAAlB,KAAoC,MAAM,WAAN,CAAkB,QAAlB,IAA8B,IAAlE,CAAvB;AACH,aAFD,MAEO;AACH,oBAAI,SAAS,EAAb;AACA,oBAAI,SAAS,EAAb;AACA,oBAAI,SAAS,EAAb;AACA,oBAAI,UAAU,EAAd;AACA,oBAAI,UAAU,EAAd;AACA,oBAAI,UAAU,EAAd;;AAEA,qBAAK,IAAI,WAAW,CAApB,EAAuB,WAAW,EAAE,MAApC,EAA4C,UAA5C,EAAwD;AACpD,wBAAI,UAAU,EAAE,QAAF,CAAd;AACA,wBAAI,UAAU,cAAc,QAAd,CAAd;AACA,wBAAI,UAAU,SAAS,QAAT,CAAd;AACA,wBAAI,KAAK,eAAL,CAAqB,OAArB,KAAiC,CAArC,EAAwC;AACpC,gCAAQ,IAAR,CAAa,OAAb;AACA,gCAAQ,IAAR,CAAa,OAAb;AACA,gCAAQ,IAAR,CAAa,OAAb;AACH,qBAJD,MAIO;AACH,+BAAO,IAAP,CAAY,OAAZ;AACA,+BAAO,IAAP,CAAY,OAAZ;AACA,+BAAO,IAAP,CAAY,OAAZ;AACH;AACJ;AACD,qBAAK,UAAL,CAAgB,2BAAhB,CAA4C,MAA5C,EAAoD,MAApD,EAA4D,MAA5D;AACA,qBAAK,WAAL,CAAiB,2BAAjB,CAA6C,OAA7C,EAAsD,OAAtD,EAA+D,OAA/D;AACH;AACJ;;;wCAEe,C,EAAG;AACf,mBAAO,EAAE,KAAK,aAAP,IAAwB,KAAK,WAA7B,GAA2C,CAA3C,GAA+C,CAAtD;AACH;;;wCAEe,C,EAAG;AACf,gBAAI,KAAK,OAAT,EAAkB;AACd,uBAAO,CAAP;AACH;AACD,gBAAI,KAAK,eAAL,CAAqB,CAArB,KAA2B,CAA/B,EAAkC;AAC9B,uBAAO,IAAI,KAAK,WAAL,CAAiB,eAAjB,CAAiC,CAAjC,CAAJ,GAA0C,CAAjD;AACH,aAFD,MAEO;AACH,uBAAO,IAAI,KAAK,UAAL,CAAgB,eAAhB,CAAgC,CAAhC,CAAX;AACH;AACJ;;;0CAEiB,C,EAAG;AACjB,gBAAI,KAAK,OAAT,EAAkB;AACd,uBAAO,KAAK,eAAZ;AACH;AACD,gBAAI,KAAK,eAAL,CAAqB,CAArB,KAA2B,CAA/B,EAAkC;AAC9B,uBAAO,KAAK,WAAL,CAAiB,iBAAjB,CAAmC,CAAnC,CAAP;AACH,aAFD,MAEO;AACH,uBAAO,KAAK,UAAL,CAAgB,iBAAhB,CAAkC,CAAlC,CAAP;AACH;AACJ;;;8CAE4B,C,EAAG,C,EAAG;AAC/B,gBAAI,QAAQ,MAAM,WAAN,CAAkB,CAAlB,CAAZ;AACA,gBAAI,YAAY,CAAC,CAAjB;AACA,gBAAI,eAAe,CAAnB;AACA,gBAAI,aAAa,CAAC,GAAlB;;AAEA,iBAAK,IAAI,UAAU,CAAnB,EAAsB,UAAU,CAAhC,EAAmC,SAAnC,EAA8C;AAC1C,oBAAI,qBAAqB,EAAzB;AACA,qBAAK,IAAI,WAAW,CAApB,EAAuB,WAAW,EAAE,MAApC,EAA4C,UAA5C,EAAwD;AACpD,uCAAmB,IAAnB,CAAwB,CAAC,EAAE,QAAF,EAAY,OAAZ,CAAD,EAAuB,EAAE,QAAF,CAAvB,CAAxB;AACH;AACD,qCAAqB,mBAAmB,IAAnB,CAAwB,UAAC,CAAD,EAAI,CAAJ;AAAA,2BAAU,EAAE,CAAF,IAAO,EAAE,CAAF,CAAjB;AAAA,iBAAxB,CAArB;;AAEA,oBAAI,WAAW,EAAf;;AAEA,qBAAK,IAAI,YAAW,CAApB,EAAuB,YAAW,mBAAmB,MAAnB,GAA4B,CAA9D,EAAiE,WAAjE,EAA6E;AACzE,gCAAY,mBAAmB,SAAnB,EAA6B,CAA7B,CAAZ;AACA,wBAAI,YAAY,QAAQ,QAAxB;AACA,wBAAI,gBAAgB,YAAW,CAA/B;AACA,wBAAI,iBAAiB,mBAAmB,MAAnB,GAA4B,aAAjD;;AAEA,wBAAI,OAAO,WAAW,QAAX,GAAsB,aAAtB,GAAsC,YAAY,SAAZ,GAAwB,cAAzE;AACA,wBAAI,OAAO,SAAX,EAAsB;AAClB,oCAAY,IAAZ;AACA,uCAAe,OAAf;AACA,qCAAa,CAAC,mBAAmB,SAAnB,EAA6B,CAA7B,IAAkC,mBAAmB,YAAW,CAA9B,EAAiC,CAAjC,CAAnC,IAA0E,EAAvF;AACH;AACJ;AACJ;AACD,mBAAO,CAAC,YAAD,EAAe,UAAf,CAAP;AACH;;;;;;IAGC,yB;AACF,uCAAY,CAAZ,EAAe,CAAf,EAAkB,YAAlB,EAAgC,SAAhC,EAA2C,aAA3C,EAA0D;AAAA;;AACtD,aAAK,SAAL,GAAiB,SAAjB;AACA,aAAK,aAAL,GAAqB,aAArB;AACA,aAAK,KAAL,GAAa,EAAb;;AAEA,YAAI,oBAAoB,IAAI,KAAJ,CAAU,EAAE,MAAZ,EAAoB,IAApB,CAAyB,EAAzB,CAAxB;;AAEA,aAAK,IAAI,UAAU,CAAnB,EAAsB,UAAU,YAAhC,EAA8C,SAA9C,EAAyD;AACrD,gBAAI,SAAS,MAAM,EAAE,MAAR,CAAb;AACA,iBAAK,IAAI,WAAW,CAApB,EAAuB,WAAW,EAAE,MAApC,EAA4C,UAA5C,EAAwD;AACpD,uBAAO,QAAP,IAAmB,EAAE,QAAF,IAAc,kBAAkB,QAAlB,CAAjC;AACH;AACD,gBAAI,WAAW,IAAI,qBAAJ,CAA0B,CAA1B,EAA6B,MAA7B,EAAqC,KAAK,SAA1C,CAAf;AACA,iBAAK,KAAL,CAAW,IAAX,CAAgB,QAAhB;;AAEA,iBAAK,IAAI,aAAW,CAApB,EAAuB,aAAW,EAAE,MAApC,EAA4C,YAA5C,EAAwD;AACpD,kCAAkB,UAAlB,KAA+B,gBAAgB,SAAS,iBAAT,CAA2B,EAAE,UAAF,CAA3B,CAA/C;AACH;AACJ;AACJ;;;;0CAEiB,C,EAAG,O,EAAS;AAC1B,sBAAU,WAAW,KAAK,KAAL,CAAW,MAAhC;AACA,gBAAI,aAAa,CAAjB;AACA,iBAAK,IAAI,UAAU,CAAnB,EAAsB,UAAU,OAAhC,EAAyC,SAAzC,EAAoD;AAChD,8BAAc,KAAK,aAAL,GAAqB,KAAK,KAAL,CAAW,OAAX,EAAoB,iBAApB,CAAsC,CAAtC,CAAnC;AACH;AACD,mBAAO,UAAP;AACH;;;;;;IAIC,0B;AACF,wCAAY,CAAZ,EAAe,CAAf,EAAkB,YAAlB,EAAgC,SAAhC,EAA2C,aAA3C,EAA0D,SAA1D,EAA0H;AAAA,YAArD,oBAAqD,uEAAhC,IAAgC;AAAA,YAA1B,kBAA0B,uEAAP,KAAO;;AAAA;;AACtH,aAAK,SAAL,GAAiB,SAAjB;AACA,aAAK,aAAL,GAAqB,aAArB;AACA,aAAK,KAAL,GAAa,EAAb;AACA,aAAK,oBAAL,GAA4B,oBAA5B;AACA,aAAK,kBAAL,GAA0B,kBAA1B;;AAEA,YAAI,oBAAoB,IAAI,KAAJ,CAAU,EAAE,MAAZ,EAAoB,IAApB,CAAyB,EAAzB,CAAxB;;AAEA,aAAK,IAAI,UAAU,CAAnB,EAAsB,UAAU,YAAhC,EAA8C,SAA9C,EAAyD;AACrD,gBAAI,SAAS,MAAM,EAAE,MAAR,CAAb;AACA,gBAAI,WAAW,MAAM,EAAE,MAAR,CAAf;AACA,iBAAK,IAAI,WAAW,CAApB,EAAuB,WAAW,EAAE,MAApC,EAA4C,UAA5C,EAAwD;AACpD,oBAAI,UAAU,2BAA2B,OAA3B,CAAmC,kBAAkB,QAAlB,CAAnC,CAAd;AACA,uBAAO,QAAP,IAAmB,EAAE,QAAF,IAAc,OAAjC;;AAEA,yBAAS,QAAT,IAAqB,IAAI,OAAJ,IAAe,IAAI,OAAnB,CAArB;AACH;;AAED,gBAAI,SAAS,MAAM,cAAN,CAAqB,CAArB,EAAwB,UAAU,KAAK,oBAAvC,CAAb;;AAVqD,uCAWb,MAAM,aAAN,CAAoB,MAApB,EAA4B,MAA5B,EAAoC,SAApC,EAA+C,OAA/C,CAXa;gBAAA;gBAWhD,YAXgD;gBAWlC,iBAXkC;;AAYrD,gBAAI,WAAW,IAAI,qBAAJ,CAA0B,YAA1B,EAAwC,iBAAxC,EAA2D,KAAK,SAAhE,CAAf;AACA,gBAAI,kBAAJ,EAAuB;AACnB,yBAAS,2BAAT,CAAqC,MAArC,EAA6C,MAA7C,EAAqD,QAArD;AACH;AACD,iBAAK,KAAL,CAAW,IAAX,CAAgB,QAAhB;;AAEA,iBAAK,IAAI,aAAW,CAApB,EAAuB,aAAW,EAAE,MAApC,EAA4C,YAA5C,EAAwD;AACpD,kCAAkB,UAAlB,KAA+B,gBAAgB,SAAS,iBAAT,CAA2B,OAAO,UAAP,CAA3B,CAA/C;AACH;AACJ;AACJ;;;;0CAMiB,C,EAAG,O,EAAS;AAC1B,sBAAU,WAAW,KAAK,KAAL,CAAW,MAAhC;AACA,gBAAI,aAAa,CAAjB;AACA,iBAAK,IAAI,UAAU,CAAnB,EAAsB,UAAU,OAAhC,EAAyC,SAAzC,EAAoD;AAChD,8BAAc,KAAK,aAAL,GAAqB,KAAK,0BAAL,CAAgC,CAAhC,EAAmC,OAAnC,CAAnC;AACH;AACD,mBAAO,2BAA2B,OAA3B,CAAmC,UAAnC,CAAP;AACH;;;mDAE0B,C,EAAG,O,EAAS;AACnC,gBAAI,gBAAgB,MAAM,YAAN,CAAmB,CAAnB,EAAsB,UAAU,KAAK,oBAArC,CAApB;AACA,mBAAO,KAAK,KAAL,CAAW,OAAX,EAAoB,iBAApB,CAAsC,aAAtC,CAAP;AACH;;;+CAEsB,C,EAAG,C,EAAG;AACzB,gBAAI,SAAS,CAAC,KAAK,GAAL,CAAS,CAAT,CAAD,CAAb;AACA,gBAAI,YAAY,MAAM,eAAN,CAAsB,KAAK,KAAL,CAAW,MAAjC,CAAhB;AACA,gBAAI,oBAAoB,IAAI,KAAJ,CAAU,EAAE,MAAZ,EAAoB,IAApB,CAAyB,CAAzB,CAAxB;AACA,iBAAK,IAAI,UAAU,CAAnB,EAAsB,UAAU,KAAK,KAAL,CAAW,MAA3C,EAAmD,SAAnD,EAA8D;AAC1D,oBAAI,OAAO,EAAX;AACA,qBAAK,IAAI,WAAW,CAApB,EAAuB,WAAW,EAAE,MAApC,EAA4C,UAA5C,EAAwD;AACpD,wBAAI,WAAW,IAAI,EAAE,QAAF,CAAJ,GAAkB,CAAjC;;AAEA,8BAAU,OAAV,EAAmB,QAAnB,IAA+B,2BAA2B,OAA3B,CAAmC,CAAE,QAAF,GAAa,kBAAkB,QAAlB,CAAhD,CAA/B;AACA,sCAAkB,QAAlB,KAA+B,KAAK,aAAL,GAAqB,KAAK,0BAAL,CAAgC,EAAE,QAAF,CAAhC,EAA6C,OAA7C,CAApD;AACA,4BAAQ,KAAK,GAAL,CAAS,IAAI,KAAK,GAAL,CAAS,CAAC,QAAD,GAAY,kBAAkB,QAAlB,CAArB,CAAb,CAAR;AACH;AACD,uBAAO,IAAP,CAAY,OAAO,EAAE,MAArB;AACH;AACD,mBAAO,CAAC,MAAD,EAAS,SAAT,CAAP;AACH;;;gCAlCc,C,EAAG;AACd,mBAAO,MAAM,IAAI,KAAK,GAAL,CAAS,CAAC,CAAV,CAAV,CAAP;AACH","file":"gradient_boosting-compiled.js","sourcesContent":["'use strict';\n\nclass DecisionTreeRegressor {\n    // slow but clear implementation of decision tree.\n    constructor(X, y, max_depth) {\n        if (max_depth == 0 || y.length < 2) {\n            this.is_leaf = true;\n            this.leaf_prediction = Utils.compute_mean(y);\n        } else {\n            this.is_leaf = false;\n            let split_and_value = DecisionTreeRegressor.compute_optimal_split(X, y);\n            this.split_feature = split_and_value[0];\n            this.split_value = split_and_value[1];\n            let X_left = [];\n            let y_left = [];\n            let X_right = [];\n            let y_right = [];\n\n            for (let event_id = 0; event_id < X.length; event_id++) {\n                let event_x = X[event_id];\n                let event_y = y[event_id];\n                if (this.predict_subtree(event_x) == 1) {\n                    X_right.push(event_x);\n                    y_right.push(event_y);\n                } else {\n                    X_left.push(event_x);\n                    y_left.push(event_y);\n                }\n            }\n            this.left_child = new DecisionTreeRegressor(X_left, y_left, max_depth - 1);\n            this.right_child = new DecisionTreeRegressor(X_right, y_right, max_depth - 1);\n        }\n    }\n\n    update_using_newton_raphson(X, antigradients, hessians){\n        if(this.is_leaf){\n            this.leaf_prediction = Utils.compute_sum(antigradients) / (Utils.compute_sum(hessians) + 1e-4);\n        } else {\n            let X_left = [];\n            let g_left = [];\n            let h_left = [];\n            let X_right = [];\n            let g_right = [];\n            let h_right = [];\n\n            for (let event_id = 0; event_id < X.length; event_id++) {\n                let event_x = X[event_id];\n                let event_g = antigradients[event_id];\n                let event_h = hessians[event_id];\n                if (this.predict_subtree(event_x) == 1) {\n                    X_right.push(event_x);\n                    g_right.push(event_g);\n                    h_right.push(event_h);\n                } else {\n                    X_left.push(event_x);\n                    g_left.push(event_g);\n                    h_left.push(event_h);\n                }\n            }\n            this.left_child.update_using_newton_raphson(X_left, g_left, h_left);\n            this.right_child.update_using_newton_raphson(X_right, g_right, h_right);\n        }\n    }\n\n    predict_subtree(x) {\n        return x[this.split_feature] > this.split_value ? 0 : 1;\n    }\n\n    predict_leaf_id(x) {\n        if (this.is_leaf) {\n            return 1;\n        }\n        if (this.predict_subtree(x) == 1) {\n            return 2 * this.right_child.predict_leaf_id(x) + 1;\n        } else {\n            return 2 * this.left_child.predict_leaf_id(x);\n        }\n    }\n\n    predict_one_event(x) {\n        if (this.is_leaf) {\n            return this.leaf_prediction;\n        }\n        if (this.predict_subtree(x) == 1) {\n            return this.right_child.predict_one_event(x);\n        } else {\n            return this.left_child.predict_one_event(x);\n        }\n    }\n\n    static compute_optimal_split(X, y) {\n        let y_sum = Utils.compute_sum(y);\n        let best_gain = -1;\n        let best_feature = 0;\n        let best_split = -999;\n\n        for (let feature = 0; feature < 2; feature++) {\n            let feature_and_answer = [];\n            for (let event_id = 0; event_id < y.length; event_id++) {\n                feature_and_answer.push([X[event_id][feature], y[event_id]]);\n            }\n            feature_and_answer = feature_and_answer.sort((a, b) => a[0] - b[0]);\n\n            let left_sum = 0.;\n\n            for (let event_id = 0; event_id < feature_and_answer.length - 1; event_id++) {\n                left_sum += feature_and_answer[event_id][1];\n                let right_sum = y_sum - left_sum;\n                let n_events_left = event_id + 1;\n                let n_events_right = feature_and_answer.length - n_events_left;\n\n                let gain = left_sum * left_sum / n_events_left + right_sum * right_sum / n_events_right;\n                if (gain > best_gain) {\n                    best_gain = gain;\n                    best_feature = feature;\n                    best_split = (feature_and_answer[event_id][0] + feature_and_answer[event_id + 1][0]) / 2.\n                }\n            }\n        }\n        return [best_feature, best_split];\n    }\n}\n\nclass GradientBoostingRegressor {\n    constructor(X, y, n_estimators, max_depth, learning_rate) {\n        this.max_depth = max_depth;\n        this.learning_rate = learning_rate;\n        this.trees = [];\n\n        let event_predictions = new Array(y.length).fill(0.);\n\n        for (let tree_id = 0; tree_id < n_estimators; tree_id++) {\n            let target = Array(y.length);\n            for (let event_id = 0; event_id < y.length; event_id++) {\n                target[event_id] = y[event_id] - event_predictions[event_id];\n            }\n            let new_tree = new DecisionTreeRegressor(X, target, this.max_depth);\n            this.trees.push(new_tree);\n\n            for (let event_id = 0; event_id < y.length; event_id++) {\n                event_predictions[event_id] += learning_rate * new_tree.predict_one_event(X[event_id]);\n            }\n        }\n    }\n\n    predict_one_event(x, n_trees) {\n        n_trees = n_trees || this.trees.length;\n        let prediction = 0;\n        for (let tree_id = 0; tree_id < n_trees; tree_id++) {\n            prediction += this.learning_rate * this.trees[tree_id].predict_one_event(x);\n        }\n        return prediction;\n    }\n}\n\n\nclass GradientBoostingClassifier {\n    constructor(X, y, n_estimators, max_depth, learning_rate, subsample, use_random_rotations=true, use_newton_raphson=false) {\n        this.max_depth = max_depth;\n        this.learning_rate = learning_rate;\n        this.trees = [];\n        this.use_random_rotations = use_random_rotations;\n        this.use_newton_raphson = use_newton_raphson;\n\n        let event_predictions = new Array(y.length).fill(0.);\n\n        for (let tree_id = 0; tree_id < n_estimators; tree_id++) {\n            let target = Array(y.length);\n            let hessians = Array(y.length);\n            for (let event_id = 0; event_id < y.length; event_id++) {\n                let sigmoid = GradientBoostingClassifier.sigmoid(event_predictions[event_id]);\n                target[event_id] = y[event_id] - sigmoid;\n                // this is wrong, but this is better for visualisation\n                hessians[event_id] = 2 * sigmoid * (1 - sigmoid);\n            }\n\n            let tree_X = Utils.rotate_dataset(X, tree_id * this.use_random_rotations);\n            let [subsampled_X, subsampled_target] = Utils.get_subsample(tree_X, target, subsample, tree_id);\n            let new_tree = new DecisionTreeRegressor(subsampled_X, subsampled_target, this.max_depth);\n            if (use_newton_raphson){\n                new_tree.update_using_newton_raphson(tree_X, target, hessians);\n            }\n            this.trees.push(new_tree);\n\n            for (let event_id = 0; event_id < y.length; event_id++) {\n                event_predictions[event_id] += learning_rate * new_tree.predict_one_event(tree_X[event_id]);\n            }\n        }\n    }\n\n    static sigmoid(x) {\n        return 1. / (1 + Math.exp(-x));\n    }\n\n    predict_one_event(x, n_trees) {\n        n_trees = n_trees || this.trees.length;\n        let prediction = 0;\n        for (let tree_id = 0; tree_id < n_trees; tree_id++) {\n            prediction += this.learning_rate * this._predict_one_event_by_tree(x, tree_id);\n        }\n        return GradientBoostingClassifier.sigmoid(prediction);\n    }\n\n    _predict_one_event_by_tree(x, tree_id) {\n        let rotated_event = Utils.rotate_event(x, tree_id * this.use_random_rotations);\n        return this.trees[tree_id].predict_one_event(rotated_event);\n    }\n\n    compute_learning_curve(X, y) {\n        let losses = [Math.log(2)];\n        let gradients = Utils.create_2D_array(this.trees.length);\n        let event_predictions = new Array(y.length).fill(0);\n        for (let tree_id = 0; tree_id < this.trees.length; tree_id++) {\n            let loss = 0.;\n            for (let event_id = 0; event_id < y.length; event_id++) {\n                let signed_y = 2 * y[event_id] - 1;\n                // important - first updating gradients, then modifying predictions.\n                gradients[tree_id][event_id] = GradientBoostingClassifier.sigmoid(- signed_y * event_predictions[event_id]);\n                event_predictions[event_id] += this.learning_rate * this._predict_one_event_by_tree(X[event_id], tree_id);\n                loss += Math.log(1 + Math.exp(-signed_y * event_predictions[event_id]));\n            }\n            losses.push(loss / y.length);\n        }\n        return [losses, gradients];\n    }\n}\n"]}