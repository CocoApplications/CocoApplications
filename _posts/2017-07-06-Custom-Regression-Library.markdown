---
layout: post
title:  "Custom Regression Library"
date: 2017-07-06 12:00:00
author: Rohan Kotwani
excerpt: "Custom library for regression algorithms"
tags: 
- Regression
- Lasso
- Ridge
- KNN
- Polynomial

---

```python
import importlib
import Regression
importlib.reload(Regression)
```





~~~python
import pandas as pd
import numpy as np

dtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int, 'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float, 'sqft_living':float, 'floors':float, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}

train = pd.read_csv("DATA/wk3_kc_house_train_data.csv",dtype=dtype_dict)
valid = pd.read_csv("DATA/wk3_kc_house_valid_data.csv",dtype=dtype_dict)
test = pd.read_csv("DATA/wk3_kc_house_test_data.csv",dtype=dtype_dict)

import heapq
def heapsort(iterable):
    h = []
    for value in iterable:
        heapq.heappush(h, value)
    return [heapq.heappop(h) for i in range(len(h))]
~~~

### Polynomial Regression


```python
heap = []
for i in range(1,15):
    z=Regression.sklearn_poly_regression(train[['sqft_living']],train[['price']],i)
    SSE = Regression.numpy_poly_regression_SSE(valid[['sqft_living']],valid[['price']],i,z)
    heap.append((SSE,i))
heapsort(heap)
```



    [(566268593826074.62, 6),
     (623955062706518.0, 2),
     (625820280251531.0, 3),
     (628240679314405.38, 5),
     (629097886299585.75, 1),
     (629987341468500.38, 4),
     (1073845577333068.5, 7),
     (7087743224421379.0, 8),
     (45303627731005880.0, 9),
     (2.4756831332531533e+17, 10),
     (7.6162300212284147e+17, 13),
     (1.1937555950312881e+18, 11),
     (2.2975609250540692e+18, 14),
     (5.1108132744530401e+18, 12)]



### Vanilla Gradient Descent


```python
X = train[['sqft_living']]
y = train[["price"]]

init_w = np.array([-47000., 1.]).reshape((2,1))
w,cost,converged = Regression.vanilla_gradient_descent(X, y, iters=1000, threshold=2.5e9, alpha=7e-12, w= init_w)
print(w,cost,converged)
```

    [[-46999.91969552]
     [   202.3576993 ]] 53434961635.5 False


### Polynomial Ridge Regression


```python
sales = pd.read_csv('DATA/kc_house_data.csv', dtype=dtype_dict)
```


```python
Regression.sklearn_ridge_poly_regression(sales[['sqft_living']],sales[['price']],degree=15,L2_penalty=1.5e-5)
```




    array([[  2.20664375e+05],
           [  1.24873306e+02],
           [ -4.77376011e-02],
           [  3.01446238e-05],
           [ -2.44419942e-09],
           [ -1.94153675e-13],
           [  8.54085686e-18],
           [  1.51142121e-21],
           [  8.27979093e-26],
           [  6.52603101e-31],
           [ -3.27895017e-34],
           [ -3.87962315e-38],
           [ -2.72437650e-42],
           [ -1.07790800e-46],
           [  3.78242694e-51],
           [  1.39790296e-54]])



### Polynomial Ridge Regression - K Fold Cross Validation - Choose L2 Penalty


```python
train_valid_shuffled = pd.read_csv('DATA/wk3_kc_house_train_valid_shuffled.csv', dtype=dtype_dict)
test = pd.read_csv('DATA/wk3_kc_house_test_data.csv', dtype=dtype_dict)

X = Regression.pandas_poly_feature(train_valid_shuffled[['sqft_living']], 15)
y = train_valid_shuffled[['price']]

heap=[]
for penalty in np.logspace(3, 9, num=13):
    SSE = Regression.sklearn_ridge_k_fold(k=10, l2_penalty=penalty, input_space = X, output = y)
    heap.append((SSE,penalty))
heapsort(heap)
```




    [(264991451592249.59, 1000.0),
     (265539188591410.62, 3162.2776601683795),
     (265885331710459.28, 10000.0),
     (265995395009836.66, 31622.776601683792),
     (266030260837924.94, 100000.0),
     (266041292470320.66, 316227.76601683791),
     (266044781588172.44, 1000000.0),
     (266045885005068.28, 3162277.6601683795),
     (266046233942223.84, 10000000.0),
     (266046344286450.69, 31622776.601683792),
     (266046379180419.94, 100000000.0),
     (266046390214868.0, 316227766.01683795),
     (266046393704267.44, 1000000000.0)]



### Ridge Regression - Gradient Descent


```python

train = pd.read_csv('DATA/kc_house_train_data.csv', dtype=dtype_dict)
test = pd.read_csv('DATA/kc_house_test_data.csv', dtype=dtype_dict)
simple_weights_0_penalty,_,_,_=Regression.ridge_gradient_descent(train[['sqft_living']].values, train[['price']], init_w=None, 
                                 alpha=1e-12,l2_penalty=0, max_iterations=1000)
simple_weights_high_penalty,_,_,_=Regression.ridge_gradient_descent(train[['sqft_living']], train[['price']], init_w=None,
                                 alpha=1e-12,l2_penalty=1e11, max_iterations=1000)

```


```python
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

sns.plt.plot(train[['sqft_living']].values,train[['price']].values,'k.',
        train[['sqft_living']].values,Regression.numpy_predict(train[['sqft_living']],simple_weights_0_penalty),'b-',
        train[['sqft_living']].values,Regression.numpy_predict(train[['sqft_living']],simple_weights_high_penalty),'r-')
plt.show()
plt.close()
```

    /Users/rohankotwani/anaconda/envs/datasci/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.
      "`IPython.html.widgets` has moved to `ipywidgets`.", ShimWarning)



![png](output_13_1.png)


### Lasso Regression - Choose L1 Penalty


```python
from math import log, sqrt

training = pd.read_csv('DATA/wk3_kc_house_train_data.csv', dtype=dtype_dict)
validation = pd.read_csv('DATA/wk3_kc_house_valid_data.csv', dtype=dtype_dict)
```


```python
training['sqft_living_sqrt'] = training['sqft_living'].apply(sqrt)
training['sqft_lot_sqrt'] = training['sqft_lot'].apply(sqrt)
training['bedrooms_square'] = training['bedrooms']*training['bedrooms']
training['floors_square'] = training['floors']*training['floors']

validation['sqft_living_sqrt'] = validation['sqft_living'].apply(sqrt)
validation['sqft_lot_sqrt'] = validation['sqft_lot'].apply(sqrt)
validation['bedrooms_square'] = validation['bedrooms']*validation['bedrooms']
validation['floors_square'] = validation['floors']*validation['floors']
```


```python
all_features = ['bedrooms', 'bedrooms_square',
            'bathrooms',
            'sqft_living', 'sqft_living_sqrt',
            'sqft_lot', 'sqft_lot_sqrt',
            'floors', 'floors_square',
            'waterfront', 'view', 'condition', 'grade',
            'sqft_above',
            'sqft_basement',
            'yr_built', 'yr_renovated']

selection = Regression.sklearn_lasso_feature_selection(training[all_features],training[['price']],l1_penalty=5e2)
sales[selection].head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sqft_living</th>
      <th>waterfront</th>
      <th>view</th>
      <th>grade</th>
      <th>yr_built</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1180.0</td>
      <td>0</td>
      <td>0</td>
      <td>7</td>
      <td>1955</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2570.0</td>
      <td>0</td>
      <td>0</td>
      <td>7</td>
      <td>1951</td>
    </tr>
    <tr>
      <th>2</th>
      <td>770.0</td>
      <td>0</td>
      <td>0</td>
      <td>6</td>
      <td>1933</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1960.0</td>
      <td>0</td>
      <td>0</td>
      <td>7</td>
      <td>1965</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1680.0</td>
      <td>0</td>
      <td>0</td>
      <td>8</td>
      <td>1987</td>
    </tr>
  </tbody>
</table>
</div>




```python

Regression.sklearn_lasso_regression(X,y,l1_penalty=5e2)

heap=[]
for l1_penalty in np.logspace(1, 7, num=13):
    z = Regression.sklearn_lasso_regression(training[all_features],training[['price']],l1_penalty)
    SSE = Regression.numpy_SSE(validation[all_features],validation[['price']],z)
    heap.append((SSE,l1_penalty))
    
heapsort(heap)
```




    [(398213327300134.88, 10.0),
     (399041900253347.0, 31.622776601683793),
     (429791604072559.56, 100.0),
     (463739831045121.38, 316.22776601683796),
     (645898733633807.0, 1000.0),
     (1222506859427163.0, 3162.2776601683795),
     (1222506859427163.0, 10000.0),
     (1222506859427163.0, 31622.776601683792),
     (1222506859427163.0, 100000.0),
     (1222506859427163.0, 316227.76601683791),
     (1222506859427163.0, 1000000.0),
     (1222506859427163.0, 3162277.6601683795),
     (1222506859427163.0, 10000000.0)]



### Lasso Regression - Get Penalty Range


```python
Regression.sklearn_lasso_penalty_range(training[all_features], training[['price']], num=7)
```




    (153.78620557013164, 206.12848615851479)



### KNN regression - Choose K


```python
train = pd.read_csv("DATA/kc_house_data_small_train.csv")
test = pd.read_csv("DATA/kc_house_data_small_test.csv")
validation = pd.read_csv("DATA/kc_house_data_validation.csv")
features=train.columns[(train.columns!='price')&(train.columns!='date')&(train.columns!='id')]
```


```python
heap=[]

for k in range(1,16):
    predictions = Regression.numpy_knn_regression(k,train[features], train[['price']], validation[features])
    SSE = np.sum((validation.price - predictions)**2)
    heap.append((SSE,k))
Regression.heapsort(heap)
```




    [(67361678735491.5, 8),
     (68338314418622.531, 7),
     (68372727958976.094, 9),
     (68903104922017.641, 6),
     (69050514340718.391, 12),
     (69333557981587.992, 10),
     (69523855215598.828, 11),
     (69846517419718.602, 5),
     (70011254508263.688, 13),
     (70911530680258.844, 14),
     (71108797486656.281, 15),
     (71934803349591.688, 4),
     (72692096019202.562, 3),
     (83445073504025.5, 2),
     (105451197751561.0, 1)]



### Gaussian Kernel Regression - Choose K & B


```python
heap=[]
for k in range(1,16):
    for b in np.linspace(0.01,0.9,16):
        predictions = Regression.numpy_gaussian_kernel_regression(k,b,train[features], train[['price']], validation[features])
        SSE = np.sum((validation.price - predictions)**2)
        heap.append((SSE,b,k))
Regression.heapsort(heap)
```




    [(63120157182714.844, 0.06933333333333333, 9),
     (63591441428473.297, 0.06933333333333333, 8),
     (63834590393153.719, 0.06933333333333333, 10),
     (64162383818952.273, 0.06933333333333333, 11),
     (64828066390168.484, 0.06933333333333333, 12),
     (64967835999588.211, 0.06933333333333333, 7),
     (65416323175709.203, 0.06933333333333333, 13),
     (65654840844454.297, 0.06933333333333333, 14),
     (65814063645579.656, 0.12866666666666668, 9),
     (65920430972726.023, 0.06933333333333333, 15),
     (66262274457003.875, 0.12866666666666668, 10),
     (66387912626907.453, 0.12866666666666668, 8),
     (66612343807296.758, 0.06933333333333333, 6),
     (66861963454357.43, 0.12866666666666668, 11),
     (67229060559788.531, 0.12866666666666668, 7),
     (67938906957667.516, 0.12866666666666668, 12),
     (68070113348132.594, 0.06933333333333333, 5),
     (68409066673834.609, 0.12866666666666668, 13),
     (68498093513983.125, 0.12866666666666668, 14),
     (68650236098431.438, 0.12866666666666668, 15),
     (68770026615140.312, 0.12866666666666668, 6),
     (70072577481522.75, 0.12866666666666668, 3),
     (70170386321585.07, 0.12866666666666668, 5),
     (70961358093648.234, 0.06933333333333333, 4),
     (71229224040041.281, 0.06933333333333333, 3),
     (72048081013666.812, 0.12866666666666668, 4),
     (73603938148117.078, 0.01, 15),
     (73671015093869.406, 0.01, 14),
     (73730024114334.125, 0.01, 13),
     (73829645784373.688, 0.01, 12),
     (74054405068501.109, 0.01, 11),
     (74255739653664.047, 0.01, 10),
     (74386169725801.688, 0.01, 9),
     (74684394046059.859, 0.01, 8),
     (75465378137888.891, 0.12866666666666668, 2),
     (75659898664754.562, 0.01, 7),
     (76613715947785.062, 0.01, 6),
     (77556419461872.281, 0.188, 9),
     (77606906958004.75, 0.01, 5),
     (77808648806097.391, 0.188, 10),
     (77834145352207.734, 0.188, 11),
     (78080502772015.031, 0.188, 8),
     (78154774307770.469, 0.188, 7),
     (79039070150092.844, 0.01, 4),
     (79070311036882.578, 0.188, 12),
     (79186746305374.469, 0.188, 6),
     (79728389511206.688, 0.188, 13),
     (79877427239594.969, 0.188, 15),
     (79932104764961.25, 0.188, 14),
     (79996603758025.406, 0.188, 5),
     (80249590358674.141, 0.06933333333333333, 2),
     (80743676360756.172, 0.188, 3),
     (80880397730429.094, 0.01, 3),
     (81417833566196.281, 0.188, 4),
     (81446143518513.031, 0.188, 2),
     (84052901355554.703, 0.188, 1),
     (85229288432839.25, 0.12866666666666668, 1),
     (88568130500256.281, 0.01, 2),
     (94243682121340.406, 0.24733333333333335, 9),
     (94389752103525.125, 0.24733333333333335, 11),
     (94473476356713.469, 0.24733333333333335, 10),
     (94909338859205.047, 0.24733333333333335, 2),
     (94937307415318.172, 0.24733333333333335, 8),
     (95034256042813.156, 0.24733333333333335, 7),
     (95101553579120.281, 0.24733333333333335, 12),
     (95301307378966.344, 0.24733333333333335, 5),
     (95309657061155.375, 0.24733333333333335, 4),
     (95352735997606.438, 0.24733333333333335, 3),
     (95389936776264.062, 0.24733333333333335, 6),
     (95507945406153.25, 0.24733333333333335, 15),
     (95576043647338.562, 0.24733333333333335, 13),
     (95706079350287.281, 0.24733333333333335, 14),
     (96424194916696.844, 0.24733333333333335, 1),
     (102831066116595.55, 0.06933333333333333, 1),
     (107949507000439.0, 0.01, 1),
     (118937825744805.22, 0.3066666666666667, 15),
     (119034868050651.3, 0.3066666666666667, 2),
     (119120950263844.25, 0.3066666666666667, 14),
     (119156181592455.75, 0.3066666666666667, 11),
     (119174731159232.58, 0.3066666666666667, 12),
     (119181168707321.22, 0.3066666666666667, 13),
     (119233181383277.72, 0.3066666666666667, 3),
     (119459352287608.59, 0.3066666666666667, 10),
     (119580265199940.66, 0.3066666666666667, 4),
     (119598305774213.48, 0.3066666666666667, 9),
     (119688401984869.31, 0.3066666666666667, 8),
     (119865491045683.7, 0.3066666666666667, 7),
     (119902451074456.31, 0.3066666666666667, 1),
     (119949065110604.17, 0.3066666666666667, 5),
     (120079694798411.22, 0.3066666666666667, 6),
     (140733379683807.14, 0.36599999999999999, 2),
     (140930251117414.31, 0.36599999999999999, 3),
     (140940413307402.38, 0.36599999999999999, 11),
     (140980044825117.31, 0.36599999999999999, 15),
     (141035349676151.19, 0.36599999999999999, 12),
     (141096595410916.53, 0.36599999999999999, 13),
     (141116344768601.34, 0.36599999999999999, 10),
     (141140592291892.78, 0.36599999999999999, 14),
     (141191063546354.47, 0.36599999999999999, 7),
     (141193461635514.88, 0.36599999999999999, 9),
     (141262120606353.12, 0.36599999999999999, 8),
     (141294930279868.69, 0.36599999999999999, 4),
     (141526054522371.88, 0.36599999999999999, 5),
     (141596276317001.34, 0.36599999999999999, 1),
     (141700033169593.56, 0.36599999999999999, 6),
     (153972645841335.28, 0.42533333333333334, 2),
     (154168082708310.12, 0.42533333333333334, 11),
     (154170504636194.59, 0.42533333333333334, 3),
     (154177949206888.44, 0.42533333333333334, 15),
     (154265245166063.69, 0.42533333333333334, 12),
     (154289351714029.94, 0.42533333333333334, 13),
     (154341129046443.62, 0.42533333333333334, 14),
     (154348366372637.72, 0.42533333333333334, 10),
     (154431528314097.66, 0.42533333333333334, 9),
     (154434496855478.88, 0.42533333333333334, 7),
     (154505142439047.56, 0.42533333333333334, 8),
     (154535286796359.0, 0.42533333333333334, 4),
     (154769296253372.69, 0.42533333333333334, 5),
     (154836738886021.59, 0.42533333333333334, 1),
     (154931694221485.12, 0.42533333333333334, 6),
     (155747300281911.5, 0.48466666666666669, 2),
     (155941973341641.81, 0.48466666666666669, 11),
     (155945412595871.41, 0.48466666666666669, 3),
     (155952096448556.41, 0.48466666666666669, 15),
     (156039780878598.12, 0.48466666666666669, 12),
     (156064087685595.31, 0.48466666666666669, 13),
     (156116192633400.31, 0.48466666666666669, 14),
     (156123252919675.31, 0.48466666666666669, 10),
     (156206925697813.38, 0.48466666666666669, 9),
     (156210152214999.56, 0.48466666666666669, 7),
     (156281002943420.69, 0.48466666666666669, 8),
     (156312115058168.72, 0.48466666666666669, 4),
     (156546649466728.75, 0.48466666666666669, 5),
     (156612300115041.62, 0.48466666666666669, 1),
     (156709542747573.28, 0.48466666666666669, 6),
     (166862463935738.31, 0.54400000000000004, 2),
     (167060406578328.12, 0.54400000000000004, 11),
     (167070732776000.69, 0.54400000000000004, 15),
     (167085175794730.41, 0.54400000000000004, 3),
     (167158720554313.12, 0.54400000000000004, 12),
     (167183185063319.88, 0.54400000000000004, 13),
     (167235546887905.25, 0.54400000000000004, 14),
     (167242465312889.38, 0.54400000000000004, 10),
     (167326537968738.25, 0.54400000000000004, 9),
     (167329962733761.31, 0.54400000000000004, 7),
     (167400973904750.91, 0.54400000000000004, 8),
     (167432847033068.81, 0.54400000000000004, 4),
     (167667794109974.31, 0.54400000000000004, 5),
     (167720372674561.06, 0.54400000000000004, 1),
     (167831069533961.09, 0.54400000000000004, 6),
     (175998863607472.19, 0.60333333333333339, 15),
     (176161102133356.25, 0.60333333333333339, 1),
     (176161102133356.25, 0.60333333333333339, 2),
     (176161102133356.25, 0.60333333333333339, 3),
     (176161102133356.25, 0.60333333333333339, 4),
     (176161102133356.25, 0.60333333333333339, 5),
     (176161102133356.25, 0.60333333333333339, 6),
     (176161102133356.25, 0.60333333333333339, 7),
     (176161102133356.25, 0.60333333333333339, 8),
     (176161102133356.25, 0.60333333333333339, 9),
     (176161102133356.25, 0.60333333333333339, 10),
     (176161102133356.25, 0.60333333333333339, 11),
     (176161102133356.25, 0.60333333333333339, 12),
     (176161102133356.25, 0.60333333333333339, 13),
     (176161102133356.25, 0.60333333333333339, 14),
     (176949352052708.03, 0.66266666666666674, 1),
     (176949352052708.03, 0.66266666666666674, 2),
     (176949352052708.03, 0.66266666666666674, 3),
     (176949352052708.03, 0.66266666666666674, 4),
     (176949352052708.03, 0.66266666666666674, 5),
     (176949352052708.03, 0.66266666666666674, 6),
     (176949352052708.03, 0.66266666666666674, 7),
     (176949352052708.03, 0.66266666666666674, 8),
     (176949352052708.03, 0.66266666666666674, 9),
     (176949352052708.03, 0.66266666666666674, 10),
     (176949352052708.03, 0.66266666666666674, 11),
     (176949352052708.03, 0.66266666666666674, 12),
     (176949352052708.03, 0.66266666666666674, 13),
     (176949352052708.03, 0.66266666666666674, 14),
     (176949352052708.03, 0.66266666666666674, 15),
     (177992309253137.66, 0.72199999999999998, 1),
     (177992309253137.66, 0.72199999999999998, 2),
     (177992309253137.66, 0.72199999999999998, 3),
     (177992309253137.66, 0.72199999999999998, 4),
     (177992309253137.66, 0.72199999999999998, 5),
     (177992309253137.66, 0.72199999999999998, 6),
     (177992309253137.66, 0.72199999999999998, 7),
     (177992309253137.66, 0.72199999999999998, 8),
     (177992309253137.66, 0.72199999999999998, 9),
     (177992309253137.66, 0.72199999999999998, 10),
     (177992309253137.66, 0.72199999999999998, 11),
     (177992309253137.66, 0.72199999999999998, 12),
     (177992309253137.66, 0.72199999999999998, 13),
     (177992309253137.66, 0.72199999999999998, 14),
     (177992309253137.66, 0.72199999999999998, 15),
     (179501275586450.25, 0.78133333333333332, 1),
     (179501275586450.25, 0.78133333333333332, 2),
     (179501275586450.25, 0.78133333333333332, 3),
     (179501275586450.25, 0.78133333333333332, 4),
     (179501275586450.25, 0.78133333333333332, 5),
     (179501275586450.25, 0.78133333333333332, 6),
     (179501275586450.25, 0.78133333333333332, 7),
     (179501275586450.25, 0.78133333333333332, 8),
     (179501275586450.25, 0.78133333333333332, 9),
     (179501275586450.25, 0.78133333333333332, 10),
     (179501275586450.25, 0.78133333333333332, 11),
     (179501275586450.25, 0.78133333333333332, 12),
     (179501275586450.25, 0.78133333333333332, 13),
     (179501275586450.25, 0.78133333333333332, 14),
     (179501275586450.25, 0.78133333333333332, 15),
     (179782084517167.56, 0.84066666666666667, 1),
     (179782084517167.56, 0.84066666666666667, 2),
     (179782084517167.56, 0.84066666666666667, 3),
     (179782084517167.56, 0.84066666666666667, 4),
     (179782084517167.56, 0.84066666666666667, 5),
     (179782084517167.56, 0.84066666666666667, 6),
     (179782084517167.56, 0.84066666666666667, 7),
     (179782084517167.56, 0.84066666666666667, 8),
     (179782084517167.56, 0.84066666666666667, 9),
     (179782084517167.56, 0.84066666666666667, 10),
     (179782084517167.56, 0.84066666666666667, 11),
     (179782084517167.56, 0.84066666666666667, 12),
     (179782084517167.56, 0.84066666666666667, 13),
     (179782084517167.56, 0.84066666666666667, 14),
     (179782084517167.56, 0.84066666666666667, 15),
     (180038037312530.81, 0.90000000000000002, 1),
     (180038037312530.81, 0.90000000000000002, 2),
     (180038037312530.81, 0.90000000000000002, 3),
     (180038037312530.81, 0.90000000000000002, 4),
     (180038037312530.81, 0.90000000000000002, 5),
     (180038037312530.81, 0.90000000000000002, 6),
     (180038037312530.81, 0.90000000000000002, 7),
     (180038037312530.81, 0.90000000000000002, 8),
     (180038037312530.81, 0.90000000000000002, 9),
     (180038037312530.81, 0.90000000000000002, 10),
     (180038037312530.81, 0.90000000000000002, 11),
     (180038037312530.81, 0.90000000000000002, 12),
     (180038037312530.81, 0.90000000000000002, 13),
     (180038037312530.81, 0.90000000000000002, 14),
     (180038037312530.81, 0.90000000000000002, 15)]



### Inverse Distance Kernel Regression - Choose K


```python
heap=[]
for k in range(1,16):
    predictions = Regression.numpy_knn_weighted_regression(k,train[features], train[['price']], validation[features])
    SSE = np.sum((validation.price - predictions)**2)
    heap.append((SSE,k))
Regression.heapsort(heap)
```




    [(66950195965431.109, 8),
     (67611010338949.328, 9),
     (67705607904796.75, 12),
     (67907048621089.789, 10),
     (67958635363273.805, 11),
     (67959958475323.359, 7),
     (68408956362761.438, 13),
     (68642904720638.234, 6),
     (69094056591556.734, 14),
     (69224109940019.086, 15),
     (69603710553541.398, 5),
     (71640084995991.656, 4),
     (72650261466114.094, 3),
     (83482557450944.938, 2),
     (105451197751561.0, 1)]




```python

```
