---
layout: post
title: Multimixture fitting
date: '2015-07-04T14:36:00.000-07:00'
author: Alex Rogozhnikov
tags:
- Machine Learning
- Graphical Models
modified_time: '2015-07-04T14:36:00.664-07:00'
blogger_id: tag:blogger.com,1999:blog-307916792578626510.post-1319838903330630919
blogger_orig_url: http://brilliantlywrong.blogspot.com/2015/07/multimixture-fitting.html
---
<p>
    I was wondering how one can modify Expectation-Maximization procedure for fitting mixtures
    (well, gaussian mixtures, because it's the only nontrivial and quite general multivariate distribution that can be fitted easily)
    to support really <b>many overlapping summands in the mixture</b>.
</p>
<p>
    Randomization probably can be a solution to this problem.
    It seems that expectation in EM algorithm is the proper place to introduce it,
    but in this case maximization ste should achieve some momentum behavior.
</p>
<p>
    Probably it is a good idea to remind how EM works.
    There are two steps that are computed iteratively:
</p>
<ol>
    <li>(Expectation) where we compute probability that each particular event belongs to each distribution</li>
    <li>(Maximization) where given the probabilities we maximize parameters of each distribution.</li>
</ol>
<p>
    What if we sample events according to distribution from expectation step?
    At each stage we will attribute each event to one (in simplest case) component of mixture, or maybe several of them.
    This kind of randomization should prevent us from 'shrinking' of distribution.
</p>
<p>
    The core idea I am trying to introduce here is very similar to dropout &mdash; a trick,
    which allowed researches to train neural networks with more parameters than amount of observations we have.
</p>
